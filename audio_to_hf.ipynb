{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 199\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 44\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 41\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_aggression = load_dataset(\"audiofolder\", data_dir=\"data/aggression\")\n",
    "dataset_aggression.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 198/198 [00:00<00:00, 814987.43files/s]\n",
      "Downloading data: 100%|██████████| 44/44 [00:00<00:00, 555871.61files/s]\n",
      "Downloading data: 100%|██████████| 43/43 [00:00<00:00, 433545.85files/s]\n",
      "Generating train split: 198 examples [00:00, 34688.28 examples/s]\n",
      "Generating validation split: 44 examples [00:00, 19272.07 examples/s]\n",
      "Generating test split: 43 examples [00:00, 21108.97 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 198\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 44\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 43\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_annoyance = load_dataset(\"audiofolder\", data_dir=\"data/annoyance\")\n",
    "dataset_annoyance.cast_column(\"audio\", Audio(sampling_rate=16000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 200/200 [00:00<00:00, 787662.72files/s]\n",
      "Downloading data: 100%|██████████| 44/44 [00:00<00:00, 369098.75files/s]\n",
      "Downloading data: 100%|██████████| 41/41 [00:00<00:00, 488541.09files/s]\n",
      "Generating train split: 200 examples [00:00, 35187.11 examples/s]\n",
      "Generating validation split: 44 examples [00:00, 19356.97 examples/s]\n",
      "Generating test split: 41 examples [00:00, 19635.36 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 44\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 41\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_frustration = load_dataset(\"audiofolder\", data_dir=\"data/frustration\")\n",
    "dataset_frustration.cast_column(\"audio\", Audio(sampling_rate=16000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 199/199 [00:00<00:00, 635.42 examples/s]it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00,  9.21ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:47<00:00, 47.20s/it]\n",
      "Map: 100%|██████████| 44/44 [00:00<00:00, 991.99 examples/s] ?it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 38.53ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:09<00:00,  9.89s/it]\n",
      "Map: 100%|██████████| 41/41 [00:00<00:00, 910.67 examples/s] ?it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 44.75ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:09<00:00,  9.61s/it]\n",
      "Map: 100%|██████████| 198/198 [00:00<00:00, 808.58 examples/s]it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00,  8.06ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:47<00:00, 47.46s/it]\n",
      "Map: 100%|██████████| 44/44 [00:00<00:00, 771.60 examples/s] ?it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 43.95ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:09<00:00,  9.63s/it]\n",
      "Map: 100%|██████████| 43/43 [00:00<00:00, 1010.01 examples/s]?it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 40.51ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:10<00:00, 10.38s/it]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 517.59 examples/s]it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00,  7.94ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:48<00:00, 48.79s/it]\n",
      "Map: 100%|██████████| 44/44 [00:00<00:00, 597.71 examples/s] ?it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 29.76ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:10<00:00, 10.01s/it]\n",
      "Map: 100%|██████████| 41/41 [00:00<00:00, 834.90 examples/s] ?it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 23.66ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:11<00:00, 11.22s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/marccgrau/vs_frustration/commit/07871555bdb5bac9e04abe424ba5e369bd9e3932', commit_message='Upload dataset', commit_description='', oid='07871555bdb5bac9e04abe424ba5e369bd9e3932', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/marccgrau/vs_frustration', endpoint='https://huggingface.co', repo_type='dataset', repo_id='marccgrau/vs_frustration'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_aggression.push_to_hub(\"marccgrau/vs_aggression\")\n",
    "dataset_annoyance.push_to_hub(\"marccgrau/vs_annoyance\")\n",
    "dataset_frustration.push_to_hub(\"marccgrau/vs_frustration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
