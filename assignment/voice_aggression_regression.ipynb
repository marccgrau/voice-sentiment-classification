{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "992fd8e8",
   "metadata": {},
   "source": [
    "# Predicting Caller Aggressiveness from Speech Features — Linear & Logistic Regression\n",
    "\n",
    "**Context.** We collected short audio snippets from customer-service style calls to study sentiment and perceived aggressiveness. These audio clips were later shown to human annotators who rated **how aggressive** the speaker sounded:\n",
    "\n",
    "- a **continuous rating** on a 1–10 scale (averaged across multiple raters); and\n",
    "- a **binary rating** indicating whether the clip was considered **aggressive (1)** or **not aggressive (0)** after aggregating multiple reviews.\n",
    "\n",
    "You are given a **preprocessed dataset** that includes:\n",
    "\n",
    "- acoustic features extracted from the audio (e.g., pitch variability, loudness, duration),\n",
    "- textual features from automatic speech recognition (ASR) using `whisper-large-v3` (e.g., transcript, word counts),\n",
    "- both aggression targets: `rating` (1–10) and `rating_binary` (0/1).\n",
    "\n",
    "Your task is to **train and interpret** linear and logistic regression models that predict caller aggressiveness based on the provided features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4c702f",
   "metadata": {},
   "source": [
    "## Dataset & Features (what they mean and how to read them)\n",
    "\n",
    "Below are the key columns you'll find in the dataset `processed_aggression_dataset.csv`:\n",
    "\n",
    "- **`file_path`**: Filename of the audio clip.\n",
    "- **`rating`** _(target for linear regression)_: Mean perceived aggressiveness (1–10). Higher means more aggressive.\n",
    "- **`rating_binary`** _(target for logistic regression)_: 1 if considered aggressive, else 0.\n",
    "\n",
    "### Acoustic features (from audio)\n",
    "\n",
    "- **`duration_sec`**: Total length of the audio clip in seconds.\n",
    "- **`mean_pitch`**: Estimated fundamental frequency (F0) in Hz; higher values generally correspond to higher perceived pitch.\n",
    "- **`pitch_var`**: Standard deviation (variability) of the pitch; higher variability can indicate more dynamic/prosodic speech.\n",
    "- **`mean_energy`**: Average root-mean-square (RMS) energy; a proxy for loudness/intensity.\n",
    "- **`spectral_centroid`**: “Brightness” of the sound (Hz); higher values indicate more energy in higher frequencies.\n",
    "\n",
    "### Textual features (from ASR transcript)\n",
    "\n",
    "- **`transcript`**: The recognized text of the audio (string).\n",
    "- **`word_count`**: Number of tokenized words in the transcript.\n",
    "- **`avg_word_length`**: Average number of characters per word.\n",
    "- **`speech_rate`**: Words per second (`word_count / duration_sec`).\n",
    "\n",
    "> **Interpretation tip:** For regression, we will mainly use numerical columns. Text can be used for qualitative interpretation, but the numerical summaries (counts, rates, durations) are what go into the models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5073442",
   "metadata": {},
   "source": [
    "## Exercise 1 — Load the data from Google Drive\n",
    "\n",
    "Load the CSV into a pandas DataFrame.\n",
    "\n",
    "> In Google Colab, Google Drive is mounted under `/content/drive`. Your file is located at:\n",
    ">\n",
    "> `/content/drive/MyDrive/[your path on the drive].csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ea883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running this in Google Colab, first mount your Drive:\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "    print(\"Drive mounted.\")\n",
    "except Exception as e:\n",
    "    print(\"Not running in Colab or Drive mount failed:\", e)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Try both common path variants just in case\n",
    "paths_to_try = [\n",
    "    \"/content/drive/MyDrive/PhD/voice_sentiment/processed_aggression_dataset.csv\",\n",
    "    \"content/drive/MyDrive/PhD/voice_sentiment/processed_aggression_dataset.csv\",\n",
    "]\n",
    "\n",
    "df = None\n",
    "for p in paths_to_try:\n",
    "    if os.path.exists(p):\n",
    "        df = pd.read_csv(p)\n",
    "        print(f\"Loaded: {p}\")\n",
    "        break\n",
    "\n",
    "if df is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find processed_aggression_dataset.csv in the expected locations.\"\n",
    "    )\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290e57c4",
   "metadata": {},
   "source": [
    "## Exercise 2 — Explore data types\n",
    "\n",
    "Inspect the structure of the DataFrame to see column names and data types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd55d9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3769dc74",
   "metadata": {},
   "source": [
    "### Exercise 2.1 — Ensure the data types are appropriate\n",
    "\n",
    "- `rating` should be `?` (continuous 1–10).\n",
    "- `rating_binary` should be `?` (0/1).\n",
    "- Feature columns should be `?`.\n",
    "- `transcript` is text.\n",
    "\n",
    "Coerce types and handle any parsing issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f6b3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected numeric columns (you can add/remove based on your dataset)\n",
    "numeric_cols = [\n",
    "    \"rating\",\n",
    "    \"rating_binary\",\n",
    "    \"duration_sec\",\n",
    "    \"mean_pitch\",\n",
    "    \"pitch_var\",\n",
    "    \"mean_energy\",\n",
    "    \"spectral_centroid\",\n",
    "    \"word_count\",\n",
    "    \"avg_word_length\",\n",
    "    \"speech_rate\",\n",
    "]\n",
    "\n",
    "# Coerce numeric columns\n",
    "for c in numeric_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# Ensure integer type for binary target\n",
    "if \"rating_binary\" in df.columns:\n",
    "    df[\"rating_binary\"] = df[\"rating_binary\"].round().astype(\"Int64\")\n",
    "\n",
    "# Quick check after coercion\n",
    "df[numeric_cols].describe(include=\"all\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b095fd47",
   "metadata": {},
   "source": [
    "### Exercise 2.2 — Explore the `rating` variable\n",
    "\n",
    "- Plot a histogram of `rating`.\n",
    "- Comment on the distribution (e.g., skew, peaks, range).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffe0359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ratings = df[\"rating\"].dropna()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(ratings, bins=10)\n",
    "plt.title(\"Distribution of Aggression Rating (1–10)\")\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Count:\", ratings.count())\n",
    "print(\"Mean:\", ratings.mean())\n",
    "print(\"Std:\", ratings.std())\n",
    "print(\"Min/Max:\", ratings.min(), ratings.max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3655a0",
   "metadata": {},
   "source": [
    "> **Discussion:**  \n",
    "> Consider whether the distribution is symmetric or skewed. Are there many low or high ratings? Does the spread suggest that predicting exact values might be challenging? Keep this in mind when interpreting the regression model and the RMSE later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dc2e81",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bbc81c4",
   "metadata": {},
   "source": [
    "### Exercise 2.3 — Relationship between `rating` and `pitch_var`\n",
    "\n",
    "Create a scatter plot of `rating` (y-axis) vs. `pitch_var` (x-axis). Comment on whether there seems to be a relationship, and what other factors might influence this relationship that **aren't** visible in the plot (e.g., loudness, speech rate, semantic content).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bef9f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[\"pitch_var\"]\n",
    "y = df[\"rating\"]\n",
    "\n",
    "mask = x.notna() & y.notna()\n",
    "x_ = x[mask]\n",
    "y_ = y[mask]\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x_, y_, alpha=0.7)\n",
    "plt.title(\"Aggression Rating vs Pitch Variability\")\n",
    "plt.xlabel(\"Pitch variability (std of F0)\")\n",
    "plt.ylabel(\"Aggression rating (1–10)\")\n",
    "plt.show()\n",
    "\n",
    "# Correlation (Pearson) as a rough summary\n",
    "corr = np.corrcoef(x_, y_)[0, 1] if len(x_) > 1 else np.nan\n",
    "print(\"Pearson correlation (rating, pitch_var):\", corr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3893a3a",
   "metadata": {},
   "source": [
    "> **Discussion:**  \n",
    "> Even if a visual trend appears, remember that correlation does not imply causation. Discuss how other factors such as **duration**, **energy (loudness)**, **speech rate**, or even **lexical content** could confound or moderate the relationship.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4365a2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b767c806",
   "metadata": {},
   "source": [
    "## Exercise 3 — Train–test split and variable separation\n",
    "\n",
    "We split the data to evaluate generalization on unseen data. A common split is **80% train / 20% test**.  \n",
    "Define **dependent (target)** and **independent (feature)** variables for the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670a7168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For linear regression target\n",
    "target_lin = \"rating\"\n",
    "features_lin = [\"mean_pitch\", \"duration_sec\", \"word_count\"]\n",
    "\n",
    "# Filter rows with complete data for selected columns\n",
    "needed_lin = [target_lin] + features_lin\n",
    "df_lin = df[needed_lin].dropna().copy()\n",
    "\n",
    "X_lin = df_lin[features_lin]\n",
    "y_lin = df_lin[target_lin]\n",
    "\n",
    "X_train_lin, X_test_lin, y_train_lin, y_test_lin = train_test_split(\n",
    "    X_lin, y_lin, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train_lin.shape, X_test_lin.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8124f69f",
   "metadata": {},
   "source": [
    "## Exercise 4 — Linear Regression (OLS)\n",
    "\n",
    "We'll model the continuous `rating` from three predictors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf601e52",
   "metadata": {},
   "source": [
    "### Exercise 4.1 — Create the model:\n",
    "\n",
    "**Formula:** `rating ~ mean_pitch + duration_sec + word_count`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97e9169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X_train_const = sm.add_constant(X_train_lin)  # adds intercept\n",
    "ols_model = sm.OLS(y_train_lin, X_train_const).fit()\n",
    "ols_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfac7e31",
   "metadata": {},
   "source": [
    "### Exercise 4.2 — Return the summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a936720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_summary = ols_model.summary()\n",
    "print(ols_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68112e5",
   "metadata": {},
   "source": [
    "### Exercise 4.3 — Interpret the model output\n",
    "\n",
    "- **Coefficients**: The sign tells you the direction (positive/negative) of association with `rating`. Magnitude indicates change in `rating` per unit increase of the predictor (holding others constant).\n",
    "- **p-values**: Indicate whether each coefficient is statistically distinguishable from zero (given assumptions).\n",
    "- **R-squared**: Percentage of variance in `rating` explained by the model (on the training set).\n",
    "- **Assumptions**: OLS assumes linearity, homoscedasticity, independence, and normally distributed residuals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2b6419",
   "metadata": {},
   "source": [
    "## Exercise 5 — Predict on the test set\n",
    "\n",
    "Use the fitted linear model to predict `rating` on the test data and evaluate with RMSE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f54bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_test_const = sm.add_constant(X_test_lin, has_constant=\"add\")\n",
    "y_pred_lin = ols_model.predict(X_test_const)\n",
    "\n",
    "rmse = mean_squared_error(y_test_lin, y_pred_lin, squared=False)\n",
    "print(\"Test RMSE:\", rmse)\n",
    "\n",
    "pd.DataFrame({\"y_true\": y_test_lin.values, \"y_pred\": y_pred_lin.values}).head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b444561",
   "metadata": {},
   "source": [
    "### Exercise 5.1 — Transform `duration_sec` and `word_count`\n",
    "\n",
    "Some features are skewed and may benefit from transformations. A simple, robust choice is **log-transform**:\n",
    "\n",
    "- Replace `duration_sec` with `log1p(duration_sec)`\n",
    "- Replace `word_count` with `log1p(word_count)`\n",
    "\n",
    "Refit the model and compare results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc42e9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformed copies\n",
    "X_train_lin_tf = X_train_lin.copy()\n",
    "X_test_lin_tf = X_test_lin.copy()\n",
    "\n",
    "for col in [\"duration_sec\", \"word_count\"]:\n",
    "    X_train_lin_tf[col] = np.log1p(X_train_lin_tf[col])\n",
    "    X_test_lin_tf[col] = np.log1p(X_test_lin_tf[col])\n",
    "\n",
    "X_train_tf_const = sm.add_constant(X_train_lin_tf)\n",
    "ols_model_tf = sm.OLS(y_train_lin, X_train_tf_const).fit()\n",
    "print(ols_model_tf.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa1f157",
   "metadata": {},
   "source": [
    "Re-evaluate RMSE on the transformed-feature model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e1a78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tf_const = sm.add_constant(X_test_lin_tf, has_constant=\"add\")\n",
    "y_pred_lin_tf = ols_model_tf.predict(X_test_tf_const)\n",
    "\n",
    "rmse_tf = mean_squared_error(y_test_lin, y_pred_lin_tf, squared=False)\n",
    "print(\"Test RMSE (transformed features):\", rmse_tf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34915e15",
   "metadata": {},
   "source": [
    "### Exercise 5.2 — Interpret the RMSE\n",
    "\n",
    "- **RMSE** is in the same units as the target (`rating`, 1–10 scale).\n",
    "- Compare RMSE to the **spread** (e.g., standard deviation) and the **range** of `rating`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237b9219",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01d1e067",
   "metadata": {},
   "source": [
    "### Exercise 5.3 — Extend the model with other variables\n",
    "\n",
    "- Try to add other variables that might improve the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa84b1dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33d464ed",
   "metadata": {},
   "source": [
    "## Exercise 6 — Logistic Regression on `rating_binary`\n",
    "\n",
    "Now predict the **binary** aggressiveness label using logistic regression. We'll use the same three predictors and the same log transforms for the skewed features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9ab660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for logistic regression\n",
    "target_log = \"rating_binary\"\n",
    "features_log = [\"mean_pitch\", \"duration_sec\", \"word_count\"]\n",
    "\n",
    "needed_log = [target_log] + features_log\n",
    "df_log = df[needed_log].dropna().copy()\n",
    "\n",
    "# Ensure binary target is 0/1 int\n",
    "df_log[target_log] = df_log[target_log].astype(int)\n",
    "\n",
    "X_log = df_log[features_log].copy()\n",
    "X_log[\"duration_sec\"] = np.log1p(X_log[\"duration_sec\"])\n",
    "X_log[\"word_count\"] = np.log1p(X_log[\"word_count\"])\n",
    "\n",
    "y_log = df_log[target_log]\n",
    "\n",
    "X_train_log, X_test_log, y_train_log, y_test_log = train_test_split(\n",
    "    X_log, y_log, test_size=0.2, random_state=42, stratify=y_log\n",
    ")\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X_train_log_const = sm.add_constant(X_train_log)\n",
    "logit_model = sm.Logit(y_train_log, X_train_log_const).fit(disp=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40526b22",
   "metadata": {},
   "source": [
    "### Exercise 6.1 — Model summary\n",
    "\n",
    "Print out the summary showing coefficient estimates on the **log-odds** scale. Positive coefficients increase the log-odds (and thus the probability) of `rating_binary = 1` (aggressive), holding other variables constant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89744bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logit_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebade928",
   "metadata": {},
   "source": [
    "### Exercise 6.2 — Compare logistic vs. linear regression\n",
    "\n",
    "- **Targets**: Linear regression predicts a **continuous** `rating` (1–10); logistic regression predicts a **probability** for `rating_binary=1`.\n",
    "- **Coefficients**: Linear coefficients are changes in the expected **rating** per unit change in a predictor; logistic coefficients are changes in **log-odds**. You can exponentiate logistic coefficients to get **odds ratios**.\n",
    "- **Metrics**: For linear we used **RMSE**; for logistic consider **accuracy**, **precision/recall**, **ROC-AUC**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d434571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "\n",
    "X_test_log_const = sm.add_constant(X_test_log, has_constant=\"add\")\n",
    "y_prob = logit_model.predict(X_test_log_const)\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "acc = accuracy_score(y_test_log, y_pred)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "    y_test_log, y_pred, average=\"binary\", zero_division=0\n",
    ")\n",
    "auc = roc_auc_score(y_test_log, y_prob)\n",
    "\n",
    "print(f\"Accuracy: {acc:.3f}\")\n",
    "print(f\"Precision: {prec:.3f}, Recall: {rec:.3f}, F1: {f1:.3f}\")\n",
    "print(f\"ROC-AUC: {auc:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
